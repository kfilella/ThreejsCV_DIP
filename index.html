<!--
<!DOCTYPE html>
<html>
	<head>
		<script language="JavaScript" src="js/jquery-2.1.4.min.js"></script>
		<script language="JavaScript" src="js/swfobject.js"></script>
		<script language="JavaScript" src="js/three.min.js"></script>
		<script language="JavaScript" src="http://mrdoob.github.com/three.js/build/three.min.js"></script>
		<script language="JavaScript" src="http://mrdoob.github.com/three.js/examples/js/Detector.js"></script>
		<script src="js/ThreeWebGL.js"></script>
		<script src="js/ThreeExtras.js"></script>
		<script src="js/RequestAnimationFrame.js"></script>
		<script language="JavaScript" src="js/scriptcam.js"></script>
		<script> 
			var SCREEN_WIDTH = 500;
			var SCREEN_HEIGHT = 384;
			var windowHalfX = SCREEN_WIDTH / 2;
			var windowHalfY = SCREEN_HEIGHT / 2;
			var FLOOR = -250;
			var container;
			var camera, scene;
			var webglRenderer;
			var mesh, zmesh, geometry;
			var mouseX = 0, mouseY = 0;
			var has_gl = 0;
			$(document).ready(function() {
				container = document.getElementById( 'obj' );
				camera = new THREE.PerspectiveCamera( 75, SCREEN_WIDTH / SCREEN_HEIGHT, 1, 100000 );
				camera.position.z = 500;
				scene = new THREE.Scene();
				var x = document.createElement( "canvas" );
				var xc = x.getContext("2d");
				x.width = x.height = 10;
				xc.fillStyle = "#EEEEEE";
				xc.fillRect(0, 0, 10, 10);
				var xm = new THREE.MeshBasicMaterial( { map: new THREE.Texture( x, new THREE.UVMapping(), THREE.RepeatWrapping, THREE.RepeatWrapping ) } );
				xm.map.needsUpdate = true;
				xm.map.repeat.set( 10, 10 );
				geometry = new THREE.PlaneGeometry( 100, 100, 15, 10 );
				mesh = new THREE.Mesh( geometry, xm );
				mesh.position.set( 0, FLOOR, 0 );
				mesh.rotation.x = - Math.PI / 2;
				mesh.scale.set( 10, 10, 10 );
				scene.add( mesh );
				var ambient = new THREE.AmbientLight( 0xffffff );
				scene.add( ambient );
				var directionalLight = new THREE.DirectionalLight( 0xffeedd );
				directionalLight.position.set( 0, 0, 255 ).normalize();
				scene.add( directionalLight );
				var directionalLight2 = new THREE.DirectionalLight( 0xffffff );
				directionalLight2.position.set( 0, 0, 100 ).normalize();
				scene.add( directionalLight2 );
				try {
					webglRenderer = new THREE.WebGLRenderer();
					webglRenderer.setSize( SCREEN_WIDTH, SCREEN_HEIGHT );
					webglRenderer.domElement.style.position = "relative";
					container.appendChild( webglRenderer.domElement );
					has_gl = 1;
				}
				catch (e) {
				}
				if (!has_gl) {
					alert("No openGL");
				}
				else {
					var loader = new THREE.JSONLoader();
					var callbackFemale = function ( geometry, materials ) {
						zmesh = new THREE.Mesh( geometry, new THREE.MeshFaceMaterial( materials ) );
						zmesh.position.set( 0, FLOOR, 150 );
						zmesh.scale.set( 3, 3, 3 );
						scene.add( zmesh );
					};
					loader.load( "3D_female.js", callbackFemale );
					animate();
				}
				$("#webcam").scriptcam({
					onMotion:onMotion,
					onError:onError,
					onWebcamReady:onWebcamReady,
					cornerRadius:0,
					zoom:0.7,
					posX:224,
					posY:0,
					flip:1
				});
			});

			function onError(errorId,errorMsg) {
				alert(errorMsg);
			}			
			function changeCamera() {
				$.scriptcam.changeCamera($('#cameraNames').val());
			}
			function getMotion() {
				$.scriptcam.getMotionParameters();
			}
			function onWebcamReady(cameraNames,camera,microphoneNames,microphone,volume) {
				$.each(cameraNames, function(index, text) {
					$('#cameraNames').append( $('<option></option>').val(index).html(text) )
				}); 
				$('#cameraNames').val(camera);
				setInterval(getMotion,500);
			}
			function onMotion(motion,brightness,color,motionx,motiony) {
				console.log(motionx,motiony);
				mouseX=parseInt(motionx*2);
				mouseY=parseInt(motiony*2);
			}
			function animate() {
				requestAnimationFrame( animate );
				camera.position.x += ( mouseX - camera.position.x ) * .05;
				camera.position.y += ( - mouseY - camera.position.y ) * .05;
				camera.lookAt( scene.position );
				webglRenderer.render( scene, camera );
			}
		</script> 
	</head>
	<body>
		<div id="obj"></div>
		<div id="webcam"></div>
		<div style="margin:5px;">
			<img src="webcamlogo.png" style="vertical-align:text-top"/>
			<select id="cameraNames" size="1" onChange="changeCamera()" style="width:245px;font-size:10px;height:25px;">
			</select>
		</div>
	</body>
</html>-->
<!doctype html>
<html lang="en">
<head>
	<title>Motion Detection</title>
	<!-- http://www.webrtc.org/ -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
	<style>
		body 
		{
			font-family: Monospace;
			font-weight: bold;
			background-color: #ccccff;
			margin: 5px;
			overflow: hidden;
		}
	</style>
</head>
<body>

<video id="monitor" autoplay style="display: none; width: 320px; height: 240px;"></video>

<div id="canvasLayers"  width="320" height="240"  style="position: relative; left: 0px; top: 0px;">
<canvas id="videoCanvas" width="320" height="240" style="z-index: 1; position: absolute; left:0px; top:0px;"></canvas>
<canvas id="layer2"     width="320" height="240" style="z-index: 2; position: absolute; left:0px; top:0px; opacity:0.5;"></canvas>
</div>
<canvas id="blendCanvas" style="display: none; position: relative; left: 320px; top: 240px; width: 320px; height: 240px;"></canvas>
<div id="messageError"></div>
<div id="messageArea" style="position: relative; left: 0px; top: 270px;">Messages will be displayed here.</div>
<div id="messageArea2" style="position: relative; left: 0px; top: 290px;">Based on 
<a href="http://www.adobe.com/devnet/html5/articles/javascript-motion-detection.html">http://www.adobe.com/devnet/html5/articles/javascript-motion-detection.html</a></div>

<script src="js/RequestAnimationFrame.js">
//  Provides requestAnimationFrame in a cross browser way.
//  http://paulirish.com/2011/requestanimationframe-for-smart-animating/
</script>

<script>
navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
window.URL = window.URL || window.webkitURL;

var camvideo = document.getElementById('monitor');

	if (!navigator.getUserMedia) 
	{
		document.getElementById('messageError').innerHTML = 
			'Sorry. <code>navigator.getUserMedia()</code> is not available.';
	}
	navigator.getUserMedia({video: true}, gotStream, noStream);

function gotStream(stream) 
{
	if (window.URL) 
	{   camvideo.src = window.URL.createObjectURL(stream);   } 
	else // Opera
	{   camvideo.src = stream;   }

	camvideo.onerror = function(e) 
	{   stream.stop();   };

	stream.onended = noStream;
}

function noStream(e) 
{
	var msg = 'No camera available.';
	if (e.code == 1) 
	{   msg = 'User denied access to use camera.';   }
	document.getElementById('errorMessage').textContent = msg;
}
</script>

<!-- The code below contains a loop to draw the contents of the video tag
	 onto the canvas tag, enabling us to do cool things with the image. -->
<!-- Based on http://www.adobe.com/devnet/html5/articles/javascript-motion-detection.html -->
<script>
// assign global variables to HTML elements
var video = document.getElementById( 'monitor' );
var videoCanvas = document.getElementById( 'videoCanvas' );
var videoContext = videoCanvas.getContext( '2d' );

var layer2Canvas = document.getElementById( 'layer2' );
var layer2Context = layer2Canvas.getContext( '2d' );

var blendCanvas  = document.getElementById( "blendCanvas" );
var blendContext = blendCanvas.getContext('2d');

var messageArea = document.getElementById( "messageArea" );

// these changes are permanent
videoContext.translate(320, 0);
videoContext.scale(-1, 1);
		
// background color if no video present
videoContext.fillStyle = '#005337';
videoContext.fillRect( 0, 0, videoCanvas.width, videoCanvas.height );				

var buttons = [];

var button1 = new Image();
button1.src ="01_-_Default1noCulling.jpg";
var buttonData1 = { name:"red", image:button1, x:320 - 96 - 30, y:10, w:32, h:32 };
buttons.push( buttonData1 );

var button2 = new Image();
button2.src ="02_-_Default1noCulling.jpg";
var buttonData2 = { name:"green", image:button2, x:320 - 64 - 20, y:10, w:32, h:32 };
buttons.push( buttonData2 );

var button3 = new Image();
button3.src ="03_-_Default1noCulling.jpg";
var buttonData3 = { name:"blue", image:button3, x:320 - 32 - 10, y:10, w:32, h:32 };
buttons.push( buttonData3 );

// start the loop				
animate();

function animate() 
{
    requestAnimationFrame( animate );
	
	render();	
	blend();	
	checkAreas();
}

function render() 
{	
	if ( video.readyState === video.HAVE_ENOUGH_DATA ) 
	{
		// mirror video
		videoContext.drawImage( video, 0, 0, videoCanvas.width, videoCanvas.height );
		for ( var i = 0; i < buttons.length; i++ )
			layer2Context.drawImage( buttons[i].image, buttons[i].x, buttons[i].y, buttons[i].w, buttons[i].h );		
	}
}

var lastImageData;

function blend() 
{
	var width  = videoCanvas.width;
	var height = videoCanvas.height;
	// get current webcam image data
	var sourceData = videoContext.getImageData(0, 0, width, height);
	// create an image if the previous image doesnï¿½t exist
	if (!lastImageData) lastImageData = videoContext.getImageData(0, 0, width, height);
	// create a ImageData instance to receive the blended result
	var blendedData = videoContext.createImageData(width, height);
	// blend the 2 images
	differenceAccuracy(blendedData.data, sourceData.data, lastImageData.data);
	// draw the result in a canvas
	blendContext.putImageData(blendedData, 0, 0);
	// store the current webcam image
	lastImageData = sourceData;
}
function differenceAccuracy(target, data1, data2) 
{
	if (data1.length != data2.length) return null;
	var i = 0;
	while (i < (data1.length * 0.25)) 
	{
		var average1 = (data1[4*i] + data1[4*i+1] + data1[4*i+2]) / 3;
		var average2 = (data2[4*i] + data2[4*i+1] + data2[4*i+2]) / 3;
		var diff = threshold(fastAbs(average1 - average2));
		target[4*i]   = diff;
		target[4*i+1] = diff;
		target[4*i+2] = diff;
		target[4*i+3] = 0xFF;
		++i;
	}
}
function fastAbs(value) 
{
	return (value ^ (value >> 31)) - (value >> 31);
}
function threshold(value) 
{
	return (value > 0x15) ? 0xFF : 0;
}

// check if white region from blend overlaps area of interest (e.g. triggers)
function checkAreas() 
{
	for (var b = 0; b < buttons.length; b++)
	{
		// get the pixels in a note area from the blended image
		var blendedData = blendContext.getImageData( buttons[b].x, buttons[b].y, buttons[b].w, buttons[b].h );
			
		// calculate the average lightness of the blended data
		var i = 0;
		var sum = 0;
		var countPixels = blendedData.data.length * 0.25;
		while (i < countPixels) 
		{
			sum += (blendedData.data[i*4] + blendedData.data[i*4+1] + blendedData.data[i*4+2]);
			++i;
		}
		// calculate an average between of the color values of the note area [0-255]
		var average = Math.round(sum / (3 * countPixels));
		if (average > 50) // more than 20% movement detected
		{
			console.log( "Button " + buttons[b].name + " triggered." ); // do stuff
			messageArea.innerHTML = "<font size='+4' color=" + buttons[b].name + "><b>Button " + buttons[b].name + " triggered.</b></font>";
		}
	}
}

</script>

</body>
</html>
